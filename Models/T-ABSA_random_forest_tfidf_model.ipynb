{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-ABSA Random Forest Model using TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries for preprocessing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some helpful libraries\n",
    "\n",
    "import os\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16131</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Calls</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16132</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Data</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - General</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Network</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          sentence1  \\\n",
       "16131  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "16132  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "16133  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "16134  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "\n",
       "           sentence2 label  \n",
       "16131    mtn - Calls  None  \n",
       "16132     mtn - Data  None  \n",
       "16133  mtn - General  None  \n",
       "16134  mtn - Network  None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'D:/Models/Random_Forest/tfidf/data/generated/'\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, \"train_NLI.tsv\"),sep=\"\\t\")\n",
    "\n",
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the reviews"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reviews = df.sentence1.str.cat(sep=' ')#function to split text into word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words in the reviews before removing any stop words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens = word_tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stop words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 100 words after removing stop words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a word cloud of the words in the reviews"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#importing the necessary libraries\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(max_words=100,background_color=\"white\").generate_from_frequencies(frequency_dist)\n",
    "plt.imshow(wordcloud,interpolation=\"bicubic\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16135, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    1\n",
       "3    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df['label'].replace(['None','Positive','Negative'],[1,2,0])\n",
    "\n",
    "y_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16131    1\n",
       "16132    1\n",
       "16133    1\n",
       "16134    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries for carrying out TFID on the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [x for x in df.columns if x != 'label']\n",
    "\n",
    "# Split the data into two dataframes (one for the labels and the other for the independent variables)\n",
    "X_data = df[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16130</th>\n",
       "      <td>1168185422222155778</td>\n",
       "      <td>shading mtn for having the worst customer serv...</td>\n",
       "      <td>mtn - Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16131</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16132</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Network</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          sentence1  \\\n",
       "16130  1168185422222155778  shading mtn for having the worst customer serv...   \n",
       "16131  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "16132  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "16133  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "16134  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "\n",
       "           sentence2  \n",
       "16130  mtn - Network  \n",
       "16131    mtn - Calls  \n",
       "16132     mtn - Data  \n",
       "16133  mtn - General  \n",
       "16134  mtn - Network  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some helpful libraries\n",
    "\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text):\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in stops]\n",
    "    \n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data['concatinated'] = X_data['concatinated'].map(transformText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtn appears: 3786 in the corpus\n"
     ]
    }
   ],
   "source": [
    "## Get the word vocabulary out of the data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_data['concatinated'])\n",
    "\n",
    "\n",
    "## Count of 'mtn' in corpus\n",
    "print ('mtn appears:', count_vect.vocabulary_.get(u'mtn') , 'in the corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of TF-IDF vector : (16135, 6762)\n"
     ]
    }
   ],
   "source": [
    "## Get the TF-IDF vector representation of the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print ('Dimension of TF-IDF vector :' , X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Fitting Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test = 'D:/Models/Random_Forest/tfidf/data/generated/'\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_dir_test, \"test_NLI.tsv\"),sep=\"\\t\")\n",
    "\n",
    "df_test['concatinated'] = df_test['sentence1'] + ' ' + df_test['sentence2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['concatinated'] = df_test['concatinated'].map(transformText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test['concatinated'].values\n",
    "\n",
    "y_test = df_test['label'].replace(['None','Positive','Negative'],[1,2,0]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction part\n",
    "\n",
    "X_new_counts = count_vect.transform(X_test)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "Y_predicted = forest.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8718455123985078\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "print(accuracy_score(y_test, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17, 0.82, 0.01],\n",
       "       [0.32, 0.66, 0.02],\n",
       "       [0.16, 0.8 , 0.04],\n",
       "       ...,\n",
       "       [0.11, 0.85, 0.04],\n",
       "       [0.08, 0.86, 0.06],\n",
       "       [0.38, 0.55, 0.07]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_forest_score = forest.predict_proba(X_new_tfidf)\n",
    "Y_forest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Open/Create a file to append data\n",
    "csvFile_pred = open('prediction_score.csv', 'w')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter_pred = csv.writer(csvFile_pred)\n",
    "\n",
    "csvWriter_pred.writerow(['predicted','score_neg','score_none','score_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(Y_predicted)):\n",
    "    csvWriter_pred.writerow([Y_predicted[f],Y_forest_score[f][0], Y_forest_score[f][1], Y_forest_score[f][2]])\n",
    "csvFile_pred.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/Create a file to append data\n",
    "csvFile_true = open('y_test_true.csv', 'w')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter_true = csv.writer(csvFile_true)\n",
    "\n",
    "csvWriter_true.writerow(['y_test'])\n",
    "\n",
    "for f in range(len(y_test)):\n",
    "    csvWriter_true.writerow([y_test[f]])\n",
    "csvFile_true.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on the save result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('y_test_true.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4557"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>score_neg</th>\n",
       "      <th>score_none</th>\n",
       "      <th>score_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4552</th>\n",
       "      <td>1</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553</th>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4556</th>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted  score_neg  score_none  score_pos\n",
       "4552          1       0.18        0.82       0.00\n",
       "4553          1       0.24        0.76       0.00\n",
       "4554          1       0.11        0.85       0.04\n",
       "4555          1       0.08        0.86       0.06\n",
       "4556          1       0.38        0.55       0.07"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('prediction_score.csv')\n",
    "dataframe.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspect_strict_Acc': 0.5399473222124671, 'aspect_Macro_F1': 0.9443483881214678, 'aspect_Macro_AUC': 0.6153760420954906, 'sentiment_Acc': 0.9580853816300129, 'sentiment_Macro_AUC': 0.7377472835679291}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_y_true():\n",
    "#    \"\"\" \n",
    "#    Read file to obtain y_true.\n",
    "#    All of five tasks of sentitel use the test set of task-BERT-pair-NLI-M to get true labels.\n",
    "#    All of five tasks of SemEval-2014 use the test set of task-BERT-pair-NLI-M to get true labels.\n",
    "#    \"\"\"\n",
    "   \n",
    "        df = pd.read_csv('y_test_true.csv')\n",
    "        y_true = []\n",
    "\n",
    "        for f in range(len(df)):\n",
    "            y_true.append(df.y_test[f])\n",
    "        \n",
    "        return y_true\n",
    "       \n",
    "#def get_y_pred(task_name, pred_data_dir):\n",
    "        \n",
    "def get_y_pred():\n",
    "#    \"\"\" \n",
    "#    Read file to obtain y_pred and scores.\n",
    "#    \"\"\"\n",
    "               \n",
    "    dataframe = pd.read_csv('prediction_score.csv')\n",
    "    \n",
    "    pred=[]\n",
    "    score=[]\n",
    "\n",
    "\n",
    "\n",
    "    for f in range(len(dataframe)):\n",
    "        pred.append(dataframe.predicted[f])\n",
    "        score.append([float(dataframe.score_pos[f]),float(dataframe.score_none[f]),float(dataframe.score_neg[f])])\n",
    "                 \n",
    "    return pred, score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentitel_strict_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate \"strict Acc\" of aspect detection task of sentitel.\n",
    "    \"\"\"\n",
    "    total_cases=int(len(y_true)/4)\n",
    "    true_cases=0\n",
    "    for i in range(total_cases):\n",
    "        if y_true[i*4]!=y_pred[i*4]:continue\n",
    "        if y_true[i*4+1]!=y_pred[i*4+1]:continue\n",
    "        if y_true[i*4+2]!=y_pred[i*4+2]:continue\n",
    "        if y_true[i*4+3]!=y_pred[i*4+3]:continue\n",
    "        true_cases+=1\n",
    "    aspect_strict_Acc = true_cases/total_cases\n",
    "\n",
    "    return aspect_strict_Acc\n",
    "\n",
    "\n",
    "def sentitel_macro_F1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-F1\" of aspect detection task of sentitel.\n",
    "    \"\"\"\n",
    "    p_all=0\n",
    "    r_all=0\n",
    "    count=0\n",
    "    for i in range(len(y_pred)//4):\n",
    "        a=set()\n",
    "        b=set()\n",
    "        for j in range(4):\n",
    "            if y_pred[i*4+j]!=0:\n",
    "                a.add(j)\n",
    "            if y_true[i*4+j]!=0:\n",
    "                b.add(j)\n",
    "        if len(b)==0:continue\n",
    "        a_b=a.intersection(b)\n",
    "        if len(a_b)>0:\n",
    "            p=len(a_b)/len(a)\n",
    "            r=len(a_b)/len(b)\n",
    "        else:\n",
    "            p=0\n",
    "            r=0\n",
    "        count+=1\n",
    "        p_all+=p\n",
    "        r_all+=r\n",
    "    Ma_p=p_all/count\n",
    "    Ma_r=r_all/count\n",
    "    aspect_Macro_F1 = 2*Ma_p*Ma_r/(Ma_p+Ma_r)\n",
    "\n",
    "    return aspect_Macro_F1\n",
    "\n",
    "\n",
    "def sentitel_AUC_Acc(y_true, score):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-AUC\" of both aspect detection and sentiment classification tasks of sentitel.\n",
    "    Calculate \"Acc\" of sentiment classification task of sentitel.\n",
    "    \"\"\"\n",
    "    # aspect-Macro-AUC\n",
    "    aspect_y_true=[]\n",
    "    aspect_y_score=[]\n",
    "    aspect_y_trues=[[],[],[],[]]\n",
    "    aspect_y_scores=[[],[],[],[]]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]>0:\n",
    "            aspect_y_true.append(0)\n",
    "        else:\n",
    "            aspect_y_true.append(1) # \"None\": 1\n",
    "        tmp_score=score[i][0] # probability of \"None\"\n",
    "        aspect_y_score.append(tmp_score)\n",
    "        aspect_y_trues[i%4].append(aspect_y_true[-1])\n",
    "        aspect_y_scores[i%4].append(aspect_y_score[-1])\n",
    "\n",
    "    aspect_auc=[]\n",
    "    for i in range(4):\n",
    "        aspect_auc.append(metrics.roc_auc_score(aspect_y_trues[i], aspect_y_scores[i]))\n",
    "    aspect_Macro_AUC = np.mean(aspect_auc)\n",
    "    \n",
    "    # sentiment-Macro-AUC\n",
    "    sentiment_y_true=[]\n",
    "    sentiment_y_pred=[]\n",
    "    sentiment_y_score=[]\n",
    "    sentiment_y_trues=[[],[],[],[]]\n",
    "    sentiment_y_scores=[[],[],[],[]]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]>0:\n",
    "            sentiment_y_true.append(y_true[i]-1) # \"Postive\":0, \"Negative\":1\n",
    "            tmp_score=score[i][2]/(score[i][1]+score[i][2])  # probability of \"Negative\"\n",
    "            sentiment_y_score.append(tmp_score)\n",
    "            if tmp_score>0.5:\n",
    "                sentiment_y_pred.append(1) # \"Negative\": 1\n",
    "            else:\n",
    "                sentiment_y_pred.append(0)\n",
    "            sentiment_y_trues[i%4].append(sentiment_y_true[-1])\n",
    "            sentiment_y_scores[i%4].append(sentiment_y_score[-1])\n",
    "\n",
    "    sentiment_auc=[]\n",
    "    for i in range(4):\n",
    "        sentiment_auc.append(metrics.roc_auc_score(sentiment_y_trues[i], sentiment_y_scores[i]))\n",
    "    sentiment_Macro_AUC = np.mean(sentiment_auc)\n",
    "\n",
    "    # sentiment Acc\n",
    "    sentiment_y_true = np.array(sentiment_y_true)\n",
    "    sentiment_y_pred = np.array(sentiment_y_pred)\n",
    "    sentiment_Acc = metrics.accuracy_score(sentiment_y_true,sentiment_y_pred)\n",
    "\n",
    "    return aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "y_true = (get_y_true())\n",
    "y_pred, score = get_y_pred()\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "\n",
    "aspect_strict_Acc = sentitel_strict_acc(y_true, y_pred)\n",
    "aspect_Macro_F1 = sentitel_macro_F1(y_true, y_pred)\n",
    "aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentitel_AUC_Acc(y_true, score)\n",
    "result = {'aspect_strict_Acc': aspect_strict_Acc,\n",
    "                'aspect_Macro_F1': aspect_Macro_F1,\n",
    "                'aspect_Macro_AUC': aspect_Macro_AUC,\n",
    "                'sentiment_Acc': sentiment_Acc,\n",
    "                'sentiment_Macro_AUC': sentiment_Macro_AUC}\n",
    "\n",
    "\n",
    "print(result)\n",
    "\n",
    "nameHandle = open('evaluation_results.txt', 'w')\n",
    "\n",
    "nameHandle.write('aspect_strict_Acc:\\t'+ str(aspect_strict_Acc))\n",
    "nameHandle.write('\\naspect_Macro_F1:\\t' + str(aspect_Macro_F1))\n",
    "nameHandle.write('\\naspect_Macro_AUC:\\t' + str(aspect_Macro_AUC))\n",
    "nameHandle.write('\\n\\nsentiment_Acc:\\t' + str(sentiment_Acc))\n",
    "nameHandle.write('\\nsentiment_Macro_AUC:\\t' + str(sentiment_Macro_AUC))\n",
    "nameHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
