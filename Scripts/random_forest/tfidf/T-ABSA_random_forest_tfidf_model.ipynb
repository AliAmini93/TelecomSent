{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-ABSA Random Forest Model using TF-IDF Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a random forest model for T-ABSA. This model is trained using the dataset. The dataset contains telecom related reviews downloaded from Twitter and Facebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20059</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - CustomerService</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20060</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Data</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20061</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - General</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Network</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          sentence1  \\\n",
       "20059  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20060  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20061  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20062  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "\n",
       "                   sentence2     label  \n",
       "20059  mtn - CustomerService  Negative  \n",
       "20060             mtn - Data      None  \n",
       "20061          mtn - General      None  \n",
       "20062          mtn - Network      None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'D:/Generate_Data/data/5_aspects/'\n",
    "df = pd.read_csv(os.path.join(data_dir, \"train_NLI.tsv\"),sep=\"\\t\")\n",
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Visualising the training set using a word cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df.sentence1.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 words in the reviews before removing any stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'the', 'airtelug', 'to', 'is', 'mtn', 'my', 'you', 'and', 'a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 100 words after removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airtelug', 'mtn', 'network', 'data', 'internet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a word cloud of the words in the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Preprocessing the training set\n",
    "\n",
    "shape of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20063, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the labels of the dataset.\n",
    "\n",
    "The label 'None' is encoded as a 1, the 'Positive' label is encoded as a 2 and the 'Negative' label is encoded as a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df['label'].replace(['None','Positive','Negative'],[1,2,0])\n",
    "\n",
    "y_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20059    0\n",
       "20060    1\n",
       "20061    1\n",
       "20062    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the labels from the reviews and target aspect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [x for x in df.columns if x != 'label']\n",
    "\n",
    "# Split the data into two dataframes (one for the labels and the other for the independent variables)\n",
    "X_data = df[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20058</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20059</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - CustomerService</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20060</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20061</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Network</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          sentence1  \\\n",
       "20058  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20059  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20060  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20061  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20062  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "\n",
       "                   sentence2  \n",
       "20058            mtn - Calls  \n",
       "20059  mtn - CustomerService  \n",
       "20060             mtn - Data  \n",
       "20061          mtn - General  \n",
       "20062          mtn - Network  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the reviews with the target-aspect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i wan na take this adamount opportunity to thank africellug for keeping me online and also my data is about to be depleted can your people talk to my people we see a way forward  africell - Calls'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data['concatinated'] = X_data['sentence1'] + ' ' + X_data['sentence2']\n",
    "#X_data['concatinated'].values\n",
    "X_data['concatinated'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_data['concatinated'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the reviews using TF-IDF and training the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries for carrying out TFID on the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some helpful libraries\n",
    "\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing the reviews before training the RF model\n",
    "\n",
    "def transformText(text):\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in stops]\n",
    "    \n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data['concatinated'] = X_data['concatinated'].map(transformText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing a couple of rows after the preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. TF-IDF Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Featurs 6768\n"
     ]
    }
   ],
   "source": [
    "## Get the word vocabulary out of the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X_train_counts = vectorizer.fit(X_data['concatinated'])\n",
    "\n",
    "num_features = len(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Number of Featurs\", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training the Random Forest classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Get the TF-IDF vector representation of the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print ('Dimension of TF-IDF vector :' , X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20063, 6768)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.transform(X_data['concatinated'])\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "Fitting Completed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Fitting Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading  and preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test set\n",
    "data_dir_test = 'D:/Generate_Data/data/5_aspects/'\n",
    "df_test = pd.read_csv(os.path.join(data_dir_test, \"test_NLI.tsv\"),sep=\"\\t\")\n",
    "df_test['concatinated'] = df_test['sentence1'] + ' ' + df_test['sentence2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['concatinated'] = df_test['concatinated'].map(transformText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test['concatinated'].values\n",
    "y_test = df_test['label'].replace(['None','Positive','Negative'],[1,2,0]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "X_test_counts = count_vect.transform(X_test)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of TF-IDF vector : (5669, 6768)\n"
     ]
    }
   ],
   "source": [
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "print ('Dimension of TF-IDF vector :' , X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "\n",
    "Y_predicted = forest.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluating the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8572940553889575\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "print(accuracy_score(y_test, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16, 0.84, 0.  ],\n",
       "       [0.15, 0.85, 0.  ],\n",
       "       [0.18, 0.8 , 0.02],\n",
       "       ...,\n",
       "       [0.15, 0.84, 0.01],\n",
       "       [0.07, 0.9 , 0.03],\n",
       "       [0.35, 0.64, 0.01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_forest_score = forest.predict_proba(X_test_tfidf)\n",
    "Y_forest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Open/Create a file to append data\n",
    "csvFile_pred = open('prediction_score.csv', 'w')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter_pred = csv.writer(csvFile_pred)\n",
    "\n",
    "csvWriter_pred.writerow(['predicted','score_none','score_pos','score_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(Y_predicted)):\n",
    "    csvWriter_pred.writerow([Y_predicted[f],Y_forest_score[f][1], Y_forest_score[f][0], Y_forest_score[f][2]])\n",
    "csvFile_pred.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on the save result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20063"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>score_none</th>\n",
       "      <th>score_pos</th>\n",
       "      <th>score_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted  score_none  score_pos  score_neg\n",
       "5664          1        0.80       0.19       0.01\n",
       "5665          1        0.87       0.12       0.01\n",
       "5666          1        0.84       0.15       0.01\n",
       "5667          1        0.90       0.07       0.03\n",
       "5668          1        0.64       0.35       0.01"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('prediction_score.csv')\n",
    "dataframe.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Performance based on the evaluation metrics\n",
    "\n",
    "AUC per Aspect\n",
    "\n",
    "aspect_strict_Accuracy\n",
    "\n",
    "aspect_Macro_F1\n",
    "\n",
    "aspect_Macro_AUC \n",
    "\n",
    "sentiment Accuracy\n",
    "\n",
    "sentiment_Macro_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5669\n",
      "AUC per aspect:\t Calls, CustomerService, Data, General, Network\n",
      "[0.8848180266940932, 0.8410555136084904, 0.8492237734055927, 0.909020618556701, 0.86456996510188]\n",
      "{'aspect_strict_Acc': 0.39452780229479256, 'aspect_Macro_F1': 0.32659151946368187, 'aspect_Macro_AUC': 0.8697375794733514, 'sentiment_Acc': 0.9670306593868123, 'sentiment_Macro_AUC': 0.9115279392081035}\n"
     ]
    }
   ],
   "source": [
    "### Code for evaluating the TABSA models\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "def get_y_true():\n",
    "#    \"\"\" \n",
    "#    Read file to obtain y_true.\n",
    "#        \n",
    "#    \"\"\"\n",
    "    true_data_file = \"D:/Generate_Data/data/5_aspects/test_NLI.tsv\"\n",
    "\n",
    "    df = pd.read_csv(true_data_file,sep='\\t')\n",
    "    y_true = []\n",
    "    for i in range(len(df)):\n",
    "        label = df['label'][i]\n",
    "        assert label in ['None', 'Positive', 'Negative'], \"error!\"\n",
    "        if label == 'None':\n",
    "            n = 1\n",
    "        elif label == 'Positive':\n",
    "            n = 2\n",
    "        else:\n",
    "            n = 0\n",
    "        y_true.append(n)\n",
    "\n",
    "    print(len(y_true))   \n",
    "    return y_true\n",
    "\n",
    "        \n",
    "def get_y_pred():\n",
    "#    \"\"\" \n",
    "#    Read file to obtain y_pred and scores.\n",
    "#    \"\"\"\n",
    "               \n",
    "    dataframe = pd.read_csv('prediction_score.csv')\n",
    "    \n",
    "    pred=[]\n",
    "    score=[]\n",
    "\n",
    "\n",
    "\n",
    "    for f in range(len(dataframe)):\n",
    "        pred.append(dataframe.predicted[f])\n",
    "        score.append([float(dataframe.score_pos[f]),float(dataframe.score_none[f]),float(dataframe.score_neg[f])])\n",
    "                 \n",
    "    return pred, score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _strict_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate \"strict Acc\" of aspect detection task dataset.\n",
    "    \"\"\"\n",
    "    total_cases=int(len(y_true)/5)\n",
    "    true_cases=0\n",
    "    for i in range(total_cases):\n",
    "        if y_true[i*5]!=y_pred[i*5]:continue\n",
    "        if y_true[i*5+1]!=y_pred[i*5+1]:continue\n",
    "        if y_true[i*5+2]!=y_pred[i*5+2]:continue\n",
    "        if y_true[i*5+3]!=y_pred[i*5+3]:continue\n",
    "        if y_true[i*5+4]!=y_pred[i*5+4]:continue\n",
    "        true_cases+=1\n",
    "    aspect_strict_Acc = true_cases/total_cases\n",
    "\n",
    "    return aspect_strict_Acc\n",
    "\n",
    "\n",
    "def _macro_F1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-F1\" of aspect detection task.\n",
    "    \"\"\"\n",
    "    p_all=0\n",
    "    r_all=0\n",
    "    count=0\n",
    "    for i in range(len(y_pred)//5):\n",
    "        a=set()\n",
    "        b=set()\n",
    "        for j in range(5):\n",
    "            if y_pred[i*5+j]!=1:\n",
    "                a.add(j)\n",
    "            if y_true[i*5+j]!=1:\n",
    "                b.add(j)\n",
    "        if len(b)==0:continue\n",
    "        a_b=a.intersection(b)\n",
    "        if len(a_b)>0:\n",
    "            p=len(a_b)/len(a)\n",
    "            r=len(a_b)/len(b)\n",
    "        else:\n",
    "            p=0\n",
    "            r=0\n",
    "        count+=1\n",
    "        p_all+=p\n",
    "        r_all+=r\n",
    "    Ma_p=p_all/count\n",
    "    Ma_r=r_all/count\n",
    "    aspect_Macro_F1 = 2*Ma_p*Ma_r/(Ma_p+Ma_r)\n",
    "\n",
    "    return aspect_Macro_F1\n",
    "\n",
    "\n",
    "def _AUC_Acc(y_true, score):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-AUC\" of both aspect detection and sentiment classification tasks.\n",
    "    Calculate \"Acc\" of sentiment classification task.\n",
    "    \"\"\"\n",
    "    # aspect-Macro-AUC\n",
    "    aspect_y_true=[]\n",
    "    aspect_y_score=[]\n",
    "    aspect_y_trues=[[],[],[],[],[]]\n",
    "    aspect_y_scores=[[],[],[],[],[]]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]>0:\n",
    "            aspect_y_true.append(0)\n",
    "        else:\n",
    "            aspect_y_true.append(1) # \"None\": 1\n",
    "        tmp_score=score[i][0] # probability of \"None\"\n",
    "        aspect_y_score.append(tmp_score)\n",
    "        aspect_y_trues[i%5].append(aspect_y_true[-1])\n",
    "        aspect_y_scores[i%5].append(aspect_y_score[-1])\n",
    "\n",
    "    aspect_auc=[]\n",
    "    for i in range(5):\n",
    "        aspect_auc.append(metrics.roc_auc_score(aspect_y_trues[i], aspect_y_scores[i]))\n",
    "        \n",
    "    print(\"AUC per aspect:\\t Calls, CustomerService, Data, General, Network\")\n",
    "    print(aspect_auc)\n",
    "    aspect_Macro_AUC = np.mean(aspect_auc)\n",
    "    \n",
    "    # sentiment-Macro-AUC\n",
    "    sentiment_y_true=[]\n",
    "    sentiment_y_pred=[]\n",
    "    sentiment_y_score=[]\n",
    "    sentiment_y_trues=[[],[],[],[],[]]\n",
    "    sentiment_y_scores=[[],[],[],[],[]]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]>0:\n",
    "            sentiment_y_true.append(y_true[i]-1) # \"Postive\":0, \"Negative\":1\n",
    "            tmp_score=score[i][2]/(score[i][1]+score[i][2])  # probability of \"Negative\"\n",
    "            sentiment_y_score.append(tmp_score)\n",
    "            if tmp_score>0.5:\n",
    "                sentiment_y_pred.append(1) # \"Negative\": 1\n",
    "            else:\n",
    "                sentiment_y_pred.append(0)\n",
    "            sentiment_y_trues[i%5].append(sentiment_y_true[-1])\n",
    "            sentiment_y_scores[i%5].append(sentiment_y_score[-1])\n",
    "\n",
    "    sentiment_auc=[]\n",
    "    for i in range(5):\n",
    "        sentiment_auc.append(metrics.roc_auc_score(sentiment_y_trues[i], sentiment_y_scores[i]))\n",
    "    sentiment_Macro_AUC = np.mean(sentiment_auc)\n",
    "\n",
    "    # sentiment Acc\n",
    "    sentiment_y_true = np.array(sentiment_y_true)\n",
    "    sentiment_y_pred = np.array(sentiment_y_pred)\n",
    "    sentiment_Acc = metrics.accuracy_score(sentiment_y_true,sentiment_y_pred)\n",
    "\n",
    "    return aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "y_true = (get_y_true())\n",
    "y_pred, score = get_y_pred()\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "\n",
    "aspect_strict_Acc = _strict_acc(y_true, y_pred)\n",
    "aspect_Macro_F1 = _macro_F1(y_true, y_pred)\n",
    "aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = _AUC_Acc(y_true, score)\n",
    "result = {'aspect_strict_Acc': aspect_strict_Acc,\n",
    "                'aspect_Macro_F1': aspect_Macro_F1,\n",
    "                'aspect_Macro_AUC': aspect_Macro_AUC,\n",
    "                'sentiment_Acc': sentiment_Acc,\n",
    "                'sentiment_Macro_AUC': sentiment_Macro_AUC}\n",
    "\n",
    "\n",
    "print(result)\n",
    "\n",
    "nameHandle = open('RF_tfidf_evaluation_results.txt', 'w')\n",
    "\n",
    "nameHandle.write('aspect_strict_Acc:\\t'+ str(aspect_strict_Acc))\n",
    "nameHandle.write('\\naspect_Macro_F1:\\t' + str(aspect_Macro_F1))\n",
    "nameHandle.write('\\naspect_Macro_AUC:\\t' + str(aspect_Macro_AUC))\n",
    "nameHandle.write('\\n\\nsentiment_Acc:\\t' + str(sentiment_Acc))\n",
    "nameHandle.write('\\nsentiment_Macro_AUC:\\t' + str(sentiment_Macro_AUC))\n",
    "nameHandle.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
