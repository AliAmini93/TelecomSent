{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-ABSA Logistic Regression Model using TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries for preprocessing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some helpful libraries\n",
    "\n",
    "import os\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20059</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - CustomerService</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20060</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Data</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20061</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - General</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Network</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          sentence1  \\\n",
       "20059  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20060  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20061  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20062  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "\n",
       "                   sentence2     label  \n",
       "20059  mtn - CustomerService  Negative  \n",
       "20060             mtn - Data      None  \n",
       "20061          mtn - General      None  \n",
       "20062          mtn - Network      None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'D:/Generate_Data/data/5_aspects/'\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, \"train_NLI.tsv\"),sep=\"\\t\")\n",
    "\n",
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20063, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df['label'].replace(['None','Positive','Negative'],[1,2,0])\n",
    "\n",
    "y_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20059    0\n",
       "20060    1\n",
       "20061    1\n",
       "20062    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries for carrying out TFID on the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [x for x in df.columns if x != 'label']\n",
    "\n",
    "# Split the data into two dataframes (one for the labels and the other for the independent variables)\n",
    "X_data = df[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20058</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20059</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - CustomerService</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20060</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20061</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>1168227041763823616</td>\n",
       "      <td>mtnugwhy did you disconnect my line from network</td>\n",
       "      <td>mtn - Network</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          sentence1  \\\n",
       "20058  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20059  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20060  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20061  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "20062  1168227041763823616  mtnugwhy did you disconnect my line from network    \n",
       "\n",
       "                   sentence2  \n",
       "20058            mtn - Calls  \n",
       "20059  mtn - CustomerService  \n",
       "20060             mtn - Data  \n",
       "20061          mtn - General  \n",
       "20062          mtn - Network  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data['concatinated'] = X_data['sentence1'] + ' ' + X_data['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_data['concatinated'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text):\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in stops]\n",
    "    \n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data['concatinated'] = X_data['concatinated'].map(transformText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF Embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Get the word vocabulary out of the data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_data['concatinated'])\n",
    "\n",
    "\n",
    "## Count of 'mtn' in corpus\n",
    "print ('mtn appears:', count_vect.vocabulary_.get(u'mtn') , 'in the corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Featurs 6768\n"
     ]
    }
   ],
   "source": [
    "## Get the word vocabulary out of the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X_train_counts = vectorizer.fit(X_data['concatinated'])\n",
    "\n",
    "num_features = len(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Number of Featurs\", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20063, 6768)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.transform(X_data['concatinated'])\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a weighted logistic regression to the labeled training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Fitting a weighted logistic regression to the labeled training data...\")\n",
    "\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model = model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Fitting Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test = 'D:/jupyter/Year2_Research/Generate_Data/data/5_aspects/'\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_dir_test, \"test_NLI.tsv\"),sep=\"\\t\")\n",
    "\n",
    "df_test['concatinated'] = df_test['sentence1'] + ' ' + df_test['sentence2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['concatinated'] = df_test['concatinated'].map(transformText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test['concatinated'].values\n",
    "y_test = df_test['label'].replace(['None','Positive','Negative'],[1,2,0]).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of TF-IDF vector : (5669, 6768)\n"
     ]
    }
   ],
   "source": [
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "print ('Dimension of TF-IDF vector :' , X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "\n",
    "Y_predicted = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7733286293878991\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "print(accuracy_score(y_test, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25258025, 0.73169552, 0.01572423],\n",
       "       [0.37322663, 0.60073771, 0.02603566],\n",
       "       [0.53269951, 0.40292008, 0.06438041],\n",
       "       ...,\n",
       "       [0.45693178, 0.4740096 , 0.06905862],\n",
       "       [0.21029751, 0.67228958, 0.11741292],\n",
       "       [0.31837616, 0.64991318, 0.03171066]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_forest_score = model.predict_proba(X_test_tfidf)\n",
    "Y_forest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Open/Create a file to append data\n",
    "csvFile_pred = open('prediction_score.csv', 'w')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter_pred = csv.writer(csvFile_pred)\n",
    "\n",
    "csvWriter_pred.writerow(['predicted','score_none','score_pos','score_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(Y_predicted)):\n",
    "    csvWriter_pred.writerow([Y_predicted[f],Y_forest_score[f][1], Y_forest_score[f][0], Y_forest_score[f][2]])\n",
    "csvFile_pred.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on the save result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20063"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>score_none</th>\n",
       "      <th>score_pos</th>\n",
       "      <th>score_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>1</td>\n",
       "      <td>0.831817</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.011982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>1</td>\n",
       "      <td>0.704573</td>\n",
       "      <td>0.272903</td>\n",
       "      <td>0.022524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>1</td>\n",
       "      <td>0.474010</td>\n",
       "      <td>0.456932</td>\n",
       "      <td>0.069059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>1</td>\n",
       "      <td>0.672290</td>\n",
       "      <td>0.210298</td>\n",
       "      <td>0.117413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>1</td>\n",
       "      <td>0.649913</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>0.031711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted  score_none  score_pos  score_neg\n",
       "5664          1    0.831817   0.156200   0.011982\n",
       "5665          1    0.704573   0.272903   0.022524\n",
       "5666          1    0.474010   0.456932   0.069059\n",
       "5667          1    0.672290   0.210298   0.117413\n",
       "5668          1    0.649913   0.318376   0.031711"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('prediction_score.csv')\n",
    "dataframe.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5669\n",
      "AUC per aspect:\t Calls, CustomerService, Data, General, Network\n",
      "[0.7427876861223441, 0.6680050524651937, 0.6631338983356692, 0.785221272315816, 0.6671451086344703]\n",
      "{'aspect_strict_Acc': 0.27184466019417475, 'aspect_Macro_F1': 0.3417650825781958, 'aspect_Macro_AUC': 0.7052586035746986, 'sentiment_Acc': 0.9428811423771525, 'sentiment_Macro_AUC': 0.8952173199906298}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "def get_y_true():\n",
    "#    \"\"\" \n",
    "#    Read file to obtain y_true.\n",
    "#        \n",
    "#    \"\"\"\n",
    "    true_data_file = \"D:/Generate_Data/data/5_aspects/test_NLI.tsv\"\n",
    "\n",
    "    df = pd.read_csv(true_data_file,sep='\\t')\n",
    "    y_true = []\n",
    "    for i in range(len(df)):\n",
    "        label = df['label'][i]\n",
    "        assert label in ['None', 'Positive', 'Negative'], \"error!\"\n",
    "        if label == 'None':\n",
    "            n = 1\n",
    "        elif label == 'Positive':\n",
    "            n = 2\n",
    "        else:\n",
    "            n = 0\n",
    "        y_true.append(n)\n",
    "\n",
    "    print(len(y_true))   \n",
    "    return y_true\n",
    "\n",
    "        \n",
    "def get_y_pred():\n",
    "#    \"\"\" \n",
    "#    Read file to obtain y_pred and scores.\n",
    "#    \"\"\"\n",
    "               \n",
    "    dataframe = pd.read_csv('prediction_score.csv')\n",
    "    \n",
    "    pred=[]\n",
    "    score=[]\n",
    "\n",
    "\n",
    "\n",
    "    for f in range(len(dataframe)):\n",
    "        pred.append(dataframe.predicted[f])\n",
    "        score.append([float(dataframe.score_pos[f]),float(dataframe.score_none[f]),float(dataframe.score_neg[f])])\n",
    "                 \n",
    "    return pred, score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _strict_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate \"strict Acc\" of aspect detection task of dataset.\n",
    "    \"\"\"\n",
    "    total_cases=int(len(y_true)/5)\n",
    "    true_cases=0\n",
    "    for i in range(total_cases):\n",
    "        if y_true[i*5]!=y_pred[i*5]:continue\n",
    "        if y_true[i*5+1]!=y_pred[i*5+1]:continue\n",
    "        if y_true[i*5+2]!=y_pred[i*5+2]:continue\n",
    "        if y_true[i*5+3]!=y_pred[i*5+3]:continue\n",
    "        if y_true[i*5+4]!=y_pred[i*5+4]:continue\n",
    "        true_cases+=1\n",
    "    aspect_strict_Acc = true_cases/total_cases\n",
    "\n",
    "    return aspect_strict_Acc\n",
    "\n",
    "\n",
    "def _macro_F1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-F1\" of aspect detection task of .\n",
    "    \"\"\"\n",
    "    p_all=0\n",
    "    r_all=0\n",
    "    count=0\n",
    "    for i in range(len(y_pred)//5):\n",
    "        a=set()\n",
    "        b=set()\n",
    "        for j in range(5):\n",
    "            if y_pred[i*5+j]!=1:\n",
    "                a.add(j)\n",
    "            if y_true[i*5+j]!=1:\n",
    "                b.add(j)\n",
    "        if len(b)==0:continue\n",
    "        a_b=a.intersection(b)\n",
    "        if len(a_b)>0:\n",
    "            p=len(a_b)/len(a)\n",
    "            r=len(a_b)/len(b)\n",
    "        else:\n",
    "            p=0\n",
    "            r=0\n",
    "        count+=1\n",
    "        p_all+=p\n",
    "        r_all+=r\n",
    "    Ma_p=p_all/count\n",
    "    Ma_r=r_all/count\n",
    "    aspect_Macro_F1 = 2*Ma_p*Ma_r/(Ma_p+Ma_r)\n",
    "\n",
    "    return aspect_Macro_F1\n",
    "\n",
    "\n",
    "def _AUC_Acc(y_true, score):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-AUC\" of both aspect detection and sentiment classification tasks of dataset.\n",
    "    Calculate \"Acc\" of sentiment classification task of dataset.\n",
    "    \"\"\"\n",
    "    # aspect-Macro-AUC\n",
    "    aspect_y_true=[]\n",
    "    aspect_y_score=[]\n",
    "    aspect_y_trues=[[],[],[],[],[]]\n",
    "    aspect_y_scores=[[],[],[],[],[]]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]>0:\n",
    "            aspect_y_true.append(0)\n",
    "        else:\n",
    "            aspect_y_true.append(1) # \"None\": 1\n",
    "        tmp_score=score[i][0] # probability of \"None\"\n",
    "        aspect_y_score.append(tmp_score)\n",
    "        aspect_y_trues[i%5].append(aspect_y_true[-1])\n",
    "        aspect_y_scores[i%5].append(aspect_y_score[-1])\n",
    "\n",
    "    aspect_auc=[]\n",
    "    for i in range(5):\n",
    "        aspect_auc.append(metrics.roc_auc_score(aspect_y_trues[i], aspect_y_scores[i]))\n",
    "        \n",
    "    print(\"AUC per aspect:\\t Calls, CustomerService, Data, General, Network\")\n",
    "    print(aspect_auc)\n",
    "    aspect_Macro_AUC = np.mean(aspect_auc)\n",
    "    \n",
    "    # sentiment-Macro-AUC\n",
    "    sentiment_y_true=[]\n",
    "    sentiment_y_pred=[]\n",
    "    sentiment_y_score=[]\n",
    "    sentiment_y_trues=[[],[],[],[],[]]\n",
    "    sentiment_y_scores=[[],[],[],[],[]]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]>0:\n",
    "            sentiment_y_true.append(y_true[i]-1) # \"Postive\":0, \"Negative\":1\n",
    "            tmp_score=score[i][2]/(score[i][1]+score[i][2])  # probability of \"Negative\"\n",
    "            sentiment_y_score.append(tmp_score)\n",
    "            if tmp_score>0.5:\n",
    "                sentiment_y_pred.append(1) # \"Negative\": 1\n",
    "            else:\n",
    "                sentiment_y_pred.append(0)\n",
    "            sentiment_y_trues[i%5].append(sentiment_y_true[-1])\n",
    "            sentiment_y_scores[i%5].append(sentiment_y_score[-1])\n",
    "\n",
    "    sentiment_auc=[]\n",
    "    for i in range(5):\n",
    "        sentiment_auc.append(metrics.roc_auc_score(sentiment_y_trues[i], sentiment_y_scores[i]))\n",
    "    sentiment_Macro_AUC = np.mean(sentiment_auc)\n",
    "\n",
    "    # sentiment Acc\n",
    "    sentiment_y_true = np.array(sentiment_y_true)\n",
    "    sentiment_y_pred = np.array(sentiment_y_pred)\n",
    "    sentiment_Acc = metrics.accuracy_score(sentiment_y_true,sentiment_y_pred)\n",
    "\n",
    "    return aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "y_true = (get_y_true())\n",
    "y_pred, score = get_y_pred()\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "\n",
    "aspect_strict_Acc = _strict_acc(y_true, y_pred)\n",
    "aspect_Macro_F1 = _macro_F1(y_true, y_pred)\n",
    "aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = _AUC_Acc(y_true, score)\n",
    "result = {'aspect_strict_Acc': aspect_strict_Acc,\n",
    "                'aspect_Macro_F1': aspect_Macro_F1,\n",
    "                'aspect_Macro_AUC': aspect_Macro_AUC,\n",
    "                'sentiment_Acc': sentiment_Acc,\n",
    "                'sentiment_Macro_AUC': sentiment_Macro_AUC}\n",
    "\n",
    "\n",
    "print(result)\n",
    "\n",
    "nameHandle = open('LR_tfidf_evaluation_results.txt', 'w')\n",
    "\n",
    "nameHandle.write('aspect_strict_Acc:\\t'+ str(aspect_strict_Acc))\n",
    "nameHandle.write('\\naspect_Macro_F1:\\t' + str(aspect_Macro_F1))\n",
    "nameHandle.write('\\naspect_Macro_AUC:\\t' + str(aspect_Macro_AUC))\n",
    "nameHandle.write('\\n\\nsentiment_Acc:\\t' + str(sentiment_Acc))\n",
    "nameHandle.write('\\nsentiment_Macro_AUC:\\t' + str(sentiment_Macro_AUC))\n",
    "nameHandle.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
